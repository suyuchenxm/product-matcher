{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f97e40",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "\n",
    "\n",
    "1. Given a product description, can it recognize whether the product is in Salty Snack category. This classifier could be helpful to identify potential competitor products for an eCommerce seller or brand. This classifier could be extended to other category or a multiclasses classifier. However, for the simplicity of the task, I only train the model to perform a binary classifier task.\n",
    "\n",
    "2. Given 2 product descriptions, the model need to classify whether they are the same product or not. This problem is a multi billions problem for a eCommerce player who sells its products across multiple platforms. Same products usually listed differently in different retailers, it becomes a challenges for seller to track the performances of the products across multiple platform with the lack of mapping tables. One of the usecase is to track the price and sales across multiple retailer so eCommerce seller can optimize their pricing strategy. Product matching could be a valuable tool for marketplace such as Amazon or Google shopping where one seller could launch multiple product listings with the exact same products and slightly different descriptions or title in order to get more market share. Human could easily tell 2 products identical even if there are slightly different phrasing or keywords in product description or differences in product pages. Therefore, a model that could detect identical products by comparing its product pages could save tons of mannual work\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97f5aa",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "There are total 113788 product descriptions scraped from different eCommerce retailers such as Amazon.com, Walmart.com, Kroger.com, Target.com, Instacrt etc. In this dataset, there are 53,788 positive label and 60,000 negative label. Positive labels are corresponding to products in Salty Snack category and negative labels are products not belong to Salty Snack category. The only feature the model will be using is the product description\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Since there is only one feature available, product description, we need to firstly vectorized the text into model learnable features. In NLP area, such vectorization process is called Word2Vec. In this exercise, I used 2 methods to encode the product descriptions \n",
    "- TFIDF\n",
    "- BERT Sentence Embedding\n",
    "\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "In this problem, I will implement 5 algorithms to classify the product into Salty Snack Category and Non-Salty Snack.\n",
    "\n",
    "- Decision Trees. \n",
    "    - Different Purning methods\n",
    "    - How to splite and select the features\n",
    "    \n",
    "\n",
    "- Neural Networks. \n",
    "    - Layers and Neurons\n",
    "    - Activation function\n",
    "    - Batch size\n",
    "    - Optimizers\n",
    "    - Termination\n",
    "\n",
    "- Boosting. \n",
    "    - Different trees\n",
    "    - Purning\n",
    "    \n",
    "\n",
    "\n",
    "- Support Vector Machines.\n",
    "    - kernel functions\n",
    "\n",
    "This should be done in such a way that you can swap out kernel functions. I'd like to see at least two.\n",
    "\n",
    "- k-Nearest Neighbors. \n",
    "\n",
    "    - Use different values of k.\n",
    "\n",
    "\n",
    "### Experiments\n",
    "### Metrics & Testing\n",
    "\n",
    "\n",
    "- the training and testing error rates you obtained running the various learning algorithms on your problems. \n",
    "- Graph of perofrmance as function of training size At the very least you should include graphs that show performance on both training and test data as a function of training size (note\n",
    "- Training time/ iterations and conversion\n",
    "- Learning curvers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525e155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATAPATH = \"/Users/surichen/Documents/Suri/GeogiaTech/Spring2023/data/project1\"\n",
    "\n",
    "data = pd.read_csv(Path(DATAPATH, \"product_categories.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04899770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    60000\n",
       "1    53788\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee348ee5",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Since we are dealing with text data, we need to firstly vectorized the text into features. There are multiple word2vec technique we could use. From simple bag of words to embedding. For the assignment purpose, I chose 2 methods\n",
    "- TF-IDF\n",
    "- Sentence-BERT embedding: Pre-trained sentence embeddings using Siameses BERT-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15edae",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb0b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from product_matcher.processor import clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9a4da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>SHORT_DESCRIPTION</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>IRI_CATEGORY_NAME</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KETO FLCK CHKN CHPS BBQ 3 OZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KETO FLCK CHKN CHPS BBQ 3 OZ</td>\n",
       "      <td>SALTY SNACKS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TITLE SHORT_DESCRIPTION  \\\n",
       "0  KETO FLCK CHKN CHPS BBQ 3 OZ               NaN   \n",
       "\n",
       "            PRODUCT_DESCRIPTION IRI_CATEGORY_NAME  label  \n",
       "0  KETO FLCK CHKN CHPS BBQ 3 OZ      SALTY SNACKS      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fb8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TITLE_CLEANED'] = data['TITLE'].apply(lambda x: clean_string(x))\n",
    "data = data.dropna(subset=['TITLE_CLEANED']).reset_index(drop=True)\n",
    "# shuffling data\n",
    "data = data.sample(frac=1)\n",
    "data.reset_index(inplace=True)\n",
    "data.to_csv(\"../data/product_categories_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea9a95",
   "metadata": {},
   "source": [
    "# Train - Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d068a38",
   "metadata": {},
   "source": [
    "1. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f153c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.75 * len(data))\n",
    "\n",
    "train = data.loc[:split,]\n",
    "test = data.loc[split:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "566a801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/problem1_training_data.csv', index=False)\n",
    "test.to_csv('../data/problem1_testing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70405c71",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Note: to fit a tfidf vectorizer, we could only use training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e7b8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = train.TITLE_CLEANED.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc554ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85332, 22966)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6101c55",
   "metadata": {},
   "source": [
    "saving vectorizer and tf_idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49eee921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(vectorizer, open(\"../product_matcher/model/tfidfvectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(X_tfidf, open(\"../data/problem1_tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff5480",
   "metadata": {},
   "source": [
    "There are total 113,775 vocabularies in this corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb26d81",
   "metadata": {},
   "source": [
    "# Sentence Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96f3fc",
   "metadata": {},
   "source": [
    "sentence embedding generates 768 dimensions of feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a751a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e6f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = sentence_embeddings = model.encode(corpus)\n",
    "pickle.dump(X_bert_train, open(\"../data/problem1_sentence_emb_train.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "108cd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = test.TITLE_CLEANED.tolist()\n",
    "X_bert_test =  model.encode(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd91b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_bert_test, open(\"../data/problem1_sentence_emb_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfca2a7",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a7f5c",
   "metadata": {},
   "source": [
    "experiments (for each model)\n",
    "- Number of training samples\n",
    "    - Accuracy\n",
    "        - F score\n",
    "        - Recalls\n",
    "        - Precisions\n",
    "    - Complexity\n",
    "        - Training/Learning time\n",
    "        - Prediction time\n",
    "    - Convergence Plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
